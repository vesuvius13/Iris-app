{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're gonna create a website where the person enters the sepallength,width,etc, and he gets the species of the flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating features\n",
    "X = iris.drop('species', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    setosa\n",
       "1    setosa\n",
       "2    setosa\n",
       "3    setosa\n",
       "4    setosa\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're gonna do multiclass classification here, so we're gonna use sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #now the labels are one hot encoded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size = 0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, activation = 'relu', input_shape = [4, ]))\n",
    "model.add(Dense(3, activation= 'softmax')) #last layer should always be equal to the number of classes\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience = 10) #to see the curve flatten out a bit longer hence, patience is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 1s 6ms/sample - loss: 1.1723 - accuracy: 0.3500 - val_loss: 1.1644 - val_accuracy: 0.3667\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 1.1670 - accuracy: 0.3500 - val_loss: 1.1597 - val_accuracy: 0.3667\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 1.1615 - accuracy: 0.3417 - val_loss: 1.1553 - val_accuracy: 0.3667\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.1560 - accuracy: 0.3333 - val_loss: 1.1508 - val_accuracy: 0.3667\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.1510 - accuracy: 0.3250 - val_loss: 1.1464 - val_accuracy: 0.3667\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 1.1463 - accuracy: 0.3333 - val_loss: 1.1418 - val_accuracy: 0.3333\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 1.1403 - accuracy: 0.3333 - val_loss: 1.1375 - val_accuracy: 0.3333\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.1355 - accuracy: 0.3333 - val_loss: 1.1333 - val_accuracy: 0.3333\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 1.1304 - accuracy: 0.3417 - val_loss: 1.1289 - val_accuracy: 0.3000\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.1253 - accuracy: 0.3333 - val_loss: 1.1247 - val_accuracy: 0.3333\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.1205 - accuracy: 0.3333 - val_loss: 1.1205 - val_accuracy: 0.3333\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.1154 - accuracy: 0.3417 - val_loss: 1.1161 - val_accuracy: 0.3333\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.1105 - accuracy: 0.3417 - val_loss: 1.1119 - val_accuracy: 0.3667\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.1058 - accuracy: 0.3500 - val_loss: 1.1078 - val_accuracy: 0.4000\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.1009 - accuracy: 0.3500 - val_loss: 1.1037 - val_accuracy: 0.4333\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 1.0965 - accuracy: 0.3500 - val_loss: 1.0996 - val_accuracy: 0.4333\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 204us/sample - loss: 1.0916 - accuracy: 0.3583 - val_loss: 1.0957 - val_accuracy: 0.4333\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 258us/sample - loss: 1.0873 - accuracy: 0.3917 - val_loss: 1.0918 - val_accuracy: 0.4333\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0828 - accuracy: 0.4167 - val_loss: 1.0880 - val_accuracy: 0.4667\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 1.0781 - accuracy: 0.4167 - val_loss: 1.0841 - val_accuracy: 0.4333\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0736 - accuracy: 0.4000 - val_loss: 1.0804 - val_accuracy: 0.4333\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 1.0692 - accuracy: 0.4417 - val_loss: 1.0765 - val_accuracy: 0.4667\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0648 - accuracy: 0.4500 - val_loss: 1.0729 - val_accuracy: 0.4667\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0601 - accuracy: 0.4500 - val_loss: 1.0690 - val_accuracy: 0.4667\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 1.0560 - accuracy: 0.4667 - val_loss: 1.0651 - val_accuracy: 0.4333\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.0513 - accuracy: 0.4833 - val_loss: 1.0613 - val_accuracy: 0.4667\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0470 - accuracy: 0.5000 - val_loss: 1.0575 - val_accuracy: 0.4667\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0427 - accuracy: 0.5667 - val_loss: 1.0538 - val_accuracy: 0.4667\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0380 - accuracy: 0.6250 - val_loss: 1.0501 - val_accuracy: 0.5000\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.0336 - accuracy: 0.6250 - val_loss: 1.0464 - val_accuracy: 0.5000\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.0293 - accuracy: 0.6417 - val_loss: 1.0427 - val_accuracy: 0.5000\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.0247 - accuracy: 0.6417 - val_loss: 1.0390 - val_accuracy: 0.5667\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 187us/sample - loss: 1.0202 - accuracy: 0.6500 - val_loss: 1.0354 - val_accuracy: 0.5333\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.0158 - accuracy: 0.6500 - val_loss: 1.0317 - val_accuracy: 0.5667\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 1.0113 - accuracy: 0.6417 - val_loss: 1.0279 - val_accuracy: 0.6333\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 1.0068 - accuracy: 0.6500 - val_loss: 1.0242 - val_accuracy: 0.6333\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 1.0023 - accuracy: 0.6500 - val_loss: 1.0206 - val_accuracy: 0.6333\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.9977 - accuracy: 0.6583 - val_loss: 1.0169 - val_accuracy: 0.6333\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.9931 - accuracy: 0.6500 - val_loss: 1.0130 - val_accuracy: 0.6333\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.9888 - accuracy: 0.6583 - val_loss: 1.0092 - val_accuracy: 0.6000\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 249us/sample - loss: 0.9841 - accuracy: 0.6667 - val_loss: 1.0055 - val_accuracy: 0.6333\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.9794 - accuracy: 0.6667 - val_loss: 1.0017 - val_accuracy: 0.6333\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.9751 - accuracy: 0.6667 - val_loss: 0.9980 - val_accuracy: 0.6333\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.9703 - accuracy: 0.6667 - val_loss: 0.9942 - val_accuracy: 0.6333\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.9658 - accuracy: 0.6667 - val_loss: 0.9903 - val_accuracy: 0.6000\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.9614 - accuracy: 0.6750 - val_loss: 0.9865 - val_accuracy: 0.6000\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.9567 - accuracy: 0.6750 - val_loss: 0.9828 - val_accuracy: 0.6000\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.9524 - accuracy: 0.6750 - val_loss: 0.9790 - val_accuracy: 0.6000\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.9474 - accuracy: 0.6750 - val_loss: 0.9749 - val_accuracy: 0.6000\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.9429 - accuracy: 0.6750 - val_loss: 0.9709 - val_accuracy: 0.6000\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.9384 - accuracy: 0.6750 - val_loss: 0.9669 - val_accuracy: 0.6000\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.9340 - accuracy: 0.6833 - val_loss: 0.9630 - val_accuracy: 0.6000\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.9293 - accuracy: 0.6833 - val_loss: 0.9591 - val_accuracy: 0.6000\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.9249 - accuracy: 0.6833 - val_loss: 0.9552 - val_accuracy: 0.6000\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.9205 - accuracy: 0.6833 - val_loss: 0.9514 - val_accuracy: 0.6000\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.9159 - accuracy: 0.6833 - val_loss: 0.9472 - val_accuracy: 0.6000\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.9114 - accuracy: 0.6833 - val_loss: 0.9432 - val_accuracy: 0.6000\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.9069 - accuracy: 0.6833 - val_loss: 0.9395 - val_accuracy: 0.6000\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.9025 - accuracy: 0.6833 - val_loss: 0.9354 - val_accuracy: 0.6000\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.8980 - accuracy: 0.6833 - val_loss: 0.9314 - val_accuracy: 0.6000\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.8936 - accuracy: 0.6833 - val_loss: 0.9274 - val_accuracy: 0.6000\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.8890 - accuracy: 0.6833 - val_loss: 0.9234 - val_accuracy: 0.6000\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.8847 - accuracy: 0.6833 - val_loss: 0.9193 - val_accuracy: 0.6000\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.8802 - accuracy: 0.6833 - val_loss: 0.9152 - val_accuracy: 0.6000\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.8760 - accuracy: 0.6833 - val_loss: 0.9110 - val_accuracy: 0.6000\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8712 - accuracy: 0.6833 - val_loss: 0.9068 - val_accuracy: 0.6000\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8668 - accuracy: 0.6833 - val_loss: 0.9028 - val_accuracy: 0.6000\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8625 - accuracy: 0.6833 - val_loss: 0.8988 - val_accuracy: 0.6000\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8580 - accuracy: 0.6833 - val_loss: 0.8947 - val_accuracy: 0.6000\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8538 - accuracy: 0.6833 - val_loss: 0.8907 - val_accuracy: 0.6000\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8492 - accuracy: 0.6833 - val_loss: 0.8865 - val_accuracy: 0.6000\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8449 - accuracy: 0.6833 - val_loss: 0.8823 - val_accuracy: 0.6000\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.8405 - accuracy: 0.6833 - val_loss: 0.8783 - val_accuracy: 0.6000\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8360 - accuracy: 0.6833 - val_loss: 0.8742 - val_accuracy: 0.6000\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8317 - accuracy: 0.6833 - val_loss: 0.8703 - val_accuracy: 0.6000\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.8274 - accuracy: 0.6833 - val_loss: 0.8664 - val_accuracy: 0.6000\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8230 - accuracy: 0.6833 - val_loss: 0.8623 - val_accuracy: 0.6000\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8187 - accuracy: 0.6833 - val_loss: 0.8581 - val_accuracy: 0.6000\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.8145 - accuracy: 0.6833 - val_loss: 0.8538 - val_accuracy: 0.6000\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.8101 - accuracy: 0.6833 - val_loss: 0.8499 - val_accuracy: 0.6000\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 258us/sample - loss: 0.8058 - accuracy: 0.6833 - val_loss: 0.8459 - val_accuracy: 0.6000\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.8014 - accuracy: 0.6833 - val_loss: 0.8421 - val_accuracy: 0.6000\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.7973 - accuracy: 0.6833 - val_loss: 0.8383 - val_accuracy: 0.6000\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.7932 - accuracy: 0.6833 - val_loss: 0.8344 - val_accuracy: 0.6000\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.7887 - accuracy: 0.6833 - val_loss: 0.8307 - val_accuracy: 0.6000\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.7845 - accuracy: 0.6833 - val_loss: 0.8271 - val_accuracy: 0.6000\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.7803 - accuracy: 0.6833 - val_loss: 0.8233 - val_accuracy: 0.6000\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.7761 - accuracy: 0.6833 - val_loss: 0.8194 - val_accuracy: 0.6000\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.7720 - accuracy: 0.6833 - val_loss: 0.8156 - val_accuracy: 0.6000\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.7678 - accuracy: 0.6833 - val_loss: 0.8119 - val_accuracy: 0.6000\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.7638 - accuracy: 0.6833 - val_loss: 0.8082 - val_accuracy: 0.6000\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.7598 - accuracy: 0.6833 - val_loss: 0.8047 - val_accuracy: 0.6000\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.7558 - accuracy: 0.6833 - val_loss: 0.8008 - val_accuracy: 0.6000\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.7516 - accuracy: 0.6833 - val_loss: 0.7971 - val_accuracy: 0.6000\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.7478 - accuracy: 0.6833 - val_loss: 0.7935 - val_accuracy: 0.6000\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.7438 - accuracy: 0.6833 - val_loss: 0.7900 - val_accuracy: 0.6000\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.7399 - accuracy: 0.6833 - val_loss: 0.7863 - val_accuracy: 0.6000\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.7361 - accuracy: 0.6833 - val_loss: 0.7829 - val_accuracy: 0.6000\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.7322 - accuracy: 0.6833 - val_loss: 0.7793 - val_accuracy: 0.6000\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.7283 - accuracy: 0.6833 - val_loss: 0.7757 - val_accuracy: 0.6000\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.7246 - accuracy: 0.6833 - val_loss: 0.7723 - val_accuracy: 0.6000\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.7208 - accuracy: 0.6833 - val_loss: 0.7687 - val_accuracy: 0.6000\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.7171 - accuracy: 0.6833 - val_loss: 0.7654 - val_accuracy: 0.6000\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.7133 - accuracy: 0.6833 - val_loss: 0.7618 - val_accuracy: 0.6000\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 249us/sample - loss: 0.7097 - accuracy: 0.6833 - val_loss: 0.7583 - val_accuracy: 0.6000\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.7060 - accuracy: 0.6833 - val_loss: 0.7548 - val_accuracy: 0.6000\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.7024 - accuracy: 0.6833 - val_loss: 0.7515 - val_accuracy: 0.6000\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.6989 - accuracy: 0.6833 - val_loss: 0.7484 - val_accuracy: 0.6000\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.6953 - accuracy: 0.6833 - val_loss: 0.7452 - val_accuracy: 0.6000\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.6919 - accuracy: 0.6833 - val_loss: 0.7422 - val_accuracy: 0.6000\n",
      "Epoch 111/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 187us/sample - loss: 0.6883 - accuracy: 0.6833 - val_loss: 0.7390 - val_accuracy: 0.6000\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.6850 - accuracy: 0.6833 - val_loss: 0.7358 - val_accuracy: 0.6000\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.6814 - accuracy: 0.6833 - val_loss: 0.7327 - val_accuracy: 0.6000\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.6781 - accuracy: 0.6833 - val_loss: 0.7297 - val_accuracy: 0.6000\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.6748 - accuracy: 0.6833 - val_loss: 0.7265 - val_accuracy: 0.6000\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.6715 - accuracy: 0.6833 - val_loss: 0.7234 - val_accuracy: 0.6000\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.6682 - accuracy: 0.6833 - val_loss: 0.7203 - val_accuracy: 0.6000\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 228us/sample - loss: 0.6650 - accuracy: 0.6833 - val_loss: 0.7172 - val_accuracy: 0.6000\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.6617 - accuracy: 0.6833 - val_loss: 0.7143 - val_accuracy: 0.6000\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.6586 - accuracy: 0.6833 - val_loss: 0.7116 - val_accuracy: 0.6000\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.6555 - accuracy: 0.6833 - val_loss: 0.7086 - val_accuracy: 0.6000\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.6524 - accuracy: 0.6833 - val_loss: 0.7059 - val_accuracy: 0.6000\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.6494 - accuracy: 0.6833 - val_loss: 0.7031 - val_accuracy: 0.6000\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.6463 - accuracy: 0.6833 - val_loss: 0.7001 - val_accuracy: 0.6000\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.6433 - accuracy: 0.6833 - val_loss: 0.6972 - val_accuracy: 0.6000\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.6404 - accuracy: 0.6833 - val_loss: 0.6945 - val_accuracy: 0.6000\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.6374 - accuracy: 0.6833 - val_loss: 0.6917 - val_accuracy: 0.6000\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.6346 - accuracy: 0.6833 - val_loss: 0.6887 - val_accuracy: 0.6000\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.6317 - accuracy: 0.6833 - val_loss: 0.6860 - val_accuracy: 0.6000\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.6289 - accuracy: 0.6833 - val_loss: 0.6833 - val_accuracy: 0.6000\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 249us/sample - loss: 0.6261 - accuracy: 0.6833 - val_loss: 0.6804 - val_accuracy: 0.6000\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.6233 - accuracy: 0.6833 - val_loss: 0.6779 - val_accuracy: 0.6000\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.6205 - accuracy: 0.6833 - val_loss: 0.6751 - val_accuracy: 0.6000\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.6179 - accuracy: 0.6833 - val_loss: 0.6721 - val_accuracy: 0.6000\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.6154 - accuracy: 0.6833 - val_loss: 0.6691 - val_accuracy: 0.6000\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 270us/sample - loss: 0.6126 - accuracy: 0.6833 - val_loss: 0.6668 - val_accuracy: 0.6000\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.6099 - accuracy: 0.6833 - val_loss: 0.6644 - val_accuracy: 0.6000\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.6074 - accuracy: 0.6833 - val_loss: 0.6621 - val_accuracy: 0.6000\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.6048 - accuracy: 0.6833 - val_loss: 0.6596 - val_accuracy: 0.6000\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.6023 - accuracy: 0.6833 - val_loss: 0.6571 - val_accuracy: 0.6000\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5998 - accuracy: 0.6833 - val_loss: 0.6544 - val_accuracy: 0.6000\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5974 - accuracy: 0.6833 - val_loss: 0.6520 - val_accuracy: 0.6000\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5949 - accuracy: 0.6833 - val_loss: 0.6496 - val_accuracy: 0.6000\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5925 - accuracy: 0.6833 - val_loss: 0.6473 - val_accuracy: 0.6000\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5901 - accuracy: 0.6833 - val_loss: 0.6450 - val_accuracy: 0.6000\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5878 - accuracy: 0.6833 - val_loss: 0.6430 - val_accuracy: 0.6000\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5855 - accuracy: 0.6833 - val_loss: 0.6407 - val_accuracy: 0.6000\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5832 - accuracy: 0.6833 - val_loss: 0.6385 - val_accuracy: 0.6000\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5809 - accuracy: 0.6833 - val_loss: 0.6364 - val_accuracy: 0.6000\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5788 - accuracy: 0.6833 - val_loss: 0.6344 - val_accuracy: 0.6000\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5765 - accuracy: 0.6833 - val_loss: 0.6321 - val_accuracy: 0.6000\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5743 - accuracy: 0.6833 - val_loss: 0.6301 - val_accuracy: 0.6000\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5721 - accuracy: 0.6833 - val_loss: 0.6278 - val_accuracy: 0.6000\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5700 - accuracy: 0.6833 - val_loss: 0.6257 - val_accuracy: 0.6000\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5679 - accuracy: 0.6833 - val_loss: 0.6238 - val_accuracy: 0.6000\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.5658 - accuracy: 0.6833 - val_loss: 0.6217 - val_accuracy: 0.6000\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.5637 - accuracy: 0.6833 - val_loss: 0.6195 - val_accuracy: 0.6000\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5617 - accuracy: 0.6833 - val_loss: 0.6175 - val_accuracy: 0.6000\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5597 - accuracy: 0.6833 - val_loss: 0.6154 - val_accuracy: 0.6000\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5577 - accuracy: 0.6833 - val_loss: 0.6133 - val_accuracy: 0.6000\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5557 - accuracy: 0.6833 - val_loss: 0.6113 - val_accuracy: 0.6000\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5538 - accuracy: 0.6833 - val_loss: 0.6092 - val_accuracy: 0.6000\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5518 - accuracy: 0.6833 - val_loss: 0.6073 - val_accuracy: 0.6000\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5499 - accuracy: 0.6833 - val_loss: 0.6051 - val_accuracy: 0.6000\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5481 - accuracy: 0.6833 - val_loss: 0.6030 - val_accuracy: 0.6000\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5464 - accuracy: 0.6833 - val_loss: 0.6008 - val_accuracy: 0.6000\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5444 - accuracy: 0.6833 - val_loss: 0.5993 - val_accuracy: 0.6000\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5425 - accuracy: 0.6833 - val_loss: 0.5974 - val_accuracy: 0.6000\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5408 - accuracy: 0.6833 - val_loss: 0.5955 - val_accuracy: 0.6000\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.5390 - accuracy: 0.6833 - val_loss: 0.5941 - val_accuracy: 0.6000\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5372 - accuracy: 0.6833 - val_loss: 0.5924 - val_accuracy: 0.6000\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 234us/sample - loss: 0.5355 - accuracy: 0.6833 - val_loss: 0.5907 - val_accuracy: 0.6000\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.5337 - accuracy: 0.6833 - val_loss: 0.5889 - val_accuracy: 0.6000\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.5320 - accuracy: 0.6833 - val_loss: 0.5871 - val_accuracy: 0.6000\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5304 - accuracy: 0.6833 - val_loss: 0.5856 - val_accuracy: 0.6000\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5287 - accuracy: 0.6833 - val_loss: 0.5838 - val_accuracy: 0.6000\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5271 - accuracy: 0.6833 - val_loss: 0.5820 - val_accuracy: 0.6000\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5254 - accuracy: 0.6833 - val_loss: 0.5803 - val_accuracy: 0.6000\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 258us/sample - loss: 0.5239 - accuracy: 0.6833 - val_loss: 0.5785 - val_accuracy: 0.6000\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5223 - accuracy: 0.6833 - val_loss: 0.5769 - val_accuracy: 0.6000\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5207 - accuracy: 0.6833 - val_loss: 0.5752 - val_accuracy: 0.6000\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5191 - accuracy: 0.6833 - val_loss: 0.5736 - val_accuracy: 0.6000\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5178 - accuracy: 0.6833 - val_loss: 0.5725 - val_accuracy: 0.6000\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5161 - accuracy: 0.6833 - val_loss: 0.5710 - val_accuracy: 0.6000\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5147 - accuracy: 0.6833 - val_loss: 0.5689 - val_accuracy: 0.6000\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5131 - accuracy: 0.6833 - val_loss: 0.5671 - val_accuracy: 0.6000\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5116 - accuracy: 0.6833 - val_loss: 0.5656 - val_accuracy: 0.6000\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5102 - accuracy: 0.6833 - val_loss: 0.5641 - val_accuracy: 0.6000\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 220us/sample - loss: 0.5087 - accuracy: 0.6833 - val_loss: 0.5629 - val_accuracy: 0.6000\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5073 - accuracy: 0.6833 - val_loss: 0.5614 - val_accuracy: 0.6000\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.5059 - accuracy: 0.6833 - val_loss: 0.5599 - val_accuracy: 0.6000\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.5045 - accuracy: 0.6833 - val_loss: 0.5584 - val_accuracy: 0.6000\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 195us/sample - loss: 0.5031 - accuracy: 0.6833 - val_loss: 0.5572 - val_accuracy: 0.6000\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.5017 - accuracy: 0.6833 - val_loss: 0.5556 - val_accuracy: 0.6000\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.5004 - accuracy: 0.6917 - val_loss: 0.5537 - val_accuracy: 0.6000\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 195us/sample - loss: 0.4990 - accuracy: 0.6917 - val_loss: 0.5524 - val_accuracy: 0.6000\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4977 - accuracy: 0.6917 - val_loss: 0.5511 - val_accuracy: 0.6000\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4963 - accuracy: 0.6917 - val_loss: 0.5495 - val_accuracy: 0.6000\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4950 - accuracy: 0.6917 - val_loss: 0.5483 - val_accuracy: 0.6000\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4937 - accuracy: 0.6917 - val_loss: 0.5467 - val_accuracy: 0.6000\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4924 - accuracy: 0.6917 - val_loss: 0.5451 - val_accuracy: 0.6000\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.4911 - accuracy: 0.6917 - val_loss: 0.5435 - val_accuracy: 0.6000\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4897 - accuracy: 0.6917 - val_loss: 0.5422 - val_accuracy: 0.6000\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4885 - accuracy: 0.6917 - val_loss: 0.5410 - val_accuracy: 0.6000\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4872 - accuracy: 0.6917 - val_loss: 0.5397 - val_accuracy: 0.6000\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4859 - accuracy: 0.6917 - val_loss: 0.5383 - val_accuracy: 0.6000\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4846 - accuracy: 0.7000 - val_loss: 0.5367 - val_accuracy: 0.6000\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4834 - accuracy: 0.7000 - val_loss: 0.5353 - val_accuracy: 0.6000\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4822 - accuracy: 0.7000 - val_loss: 0.5341 - val_accuracy: 0.6000\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.4809 - accuracy: 0.7000 - val_loss: 0.5326 - val_accuracy: 0.6000\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4798 - accuracy: 0.7000 - val_loss: 0.5311 - val_accuracy: 0.6000\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4787 - accuracy: 0.7000 - val_loss: 0.5295 - val_accuracy: 0.6000\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4775 - accuracy: 0.7000 - val_loss: 0.5287 - val_accuracy: 0.6000\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4762 - accuracy: 0.7000 - val_loss: 0.5275 - val_accuracy: 0.6000\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4751 - accuracy: 0.7000 - val_loss: 0.5263 - val_accuracy: 0.6000\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4740 - accuracy: 0.7000 - val_loss: 0.5248 - val_accuracy: 0.6000\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4729 - accuracy: 0.7000 - val_loss: 0.5236 - val_accuracy: 0.6000\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4717 - accuracy: 0.7000 - val_loss: 0.5223 - val_accuracy: 0.6000\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4706 - accuracy: 0.7167 - val_loss: 0.5210 - val_accuracy: 0.6000\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4695 - accuracy: 0.7250 - val_loss: 0.5198 - val_accuracy: 0.6000\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4685 - accuracy: 0.7250 - val_loss: 0.5182 - val_accuracy: 0.6000\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4673 - accuracy: 0.7250 - val_loss: 0.5171 - val_accuracy: 0.6000\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4663 - accuracy: 0.7250 - val_loss: 0.5158 - val_accuracy: 0.6000\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4652 - accuracy: 0.7250 - val_loss: 0.5146 - val_accuracy: 0.6333\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4641 - accuracy: 0.7250 - val_loss: 0.5135 - val_accuracy: 0.6333\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4631 - accuracy: 0.7250 - val_loss: 0.5123 - val_accuracy: 0.6333\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4621 - accuracy: 0.7250 - val_loss: 0.5115 - val_accuracy: 0.6333\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4611 - accuracy: 0.7250 - val_loss: 0.5107 - val_accuracy: 0.6333\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4600 - accuracy: 0.7250 - val_loss: 0.5095 - val_accuracy: 0.6333\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4591 - accuracy: 0.7250 - val_loss: 0.5081 - val_accuracy: 0.6333\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4580 - accuracy: 0.7250 - val_loss: 0.5073 - val_accuracy: 0.6333\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4569 - accuracy: 0.7250 - val_loss: 0.5062 - val_accuracy: 0.6333\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4559 - accuracy: 0.7333 - val_loss: 0.5051 - val_accuracy: 0.6333\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4550 - accuracy: 0.7333 - val_loss: 0.5040 - val_accuracy: 0.6333\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4539 - accuracy: 0.7333 - val_loss: 0.5031 - val_accuracy: 0.6333\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4530 - accuracy: 0.7333 - val_loss: 0.5021 - val_accuracy: 0.6333\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 247us/sample - loss: 0.4520 - accuracy: 0.7333 - val_loss: 0.5011 - val_accuracy: 0.6333\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.4510 - accuracy: 0.7417 - val_loss: 0.5002 - val_accuracy: 0.6333\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4501 - accuracy: 0.7417 - val_loss: 0.4991 - val_accuracy: 0.6333\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.4491 - accuracy: 0.7417 - val_loss: 0.4983 - val_accuracy: 0.6333\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4482 - accuracy: 0.7417 - val_loss: 0.4972 - val_accuracy: 0.6333\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 249us/sample - loss: 0.4472 - accuracy: 0.7417 - val_loss: 0.4962 - val_accuracy: 0.6333\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4463 - accuracy: 0.7500 - val_loss: 0.4952 - val_accuracy: 0.6333\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4454 - accuracy: 0.7500 - val_loss: 0.4940 - val_accuracy: 0.6333\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.4445 - accuracy: 0.7500 - val_loss: 0.4930 - val_accuracy: 0.6333\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.4435 - accuracy: 0.7500 - val_loss: 0.4921 - val_accuracy: 0.6333\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4427 - accuracy: 0.7500 - val_loss: 0.4909 - val_accuracy: 0.6667\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.4417 - accuracy: 0.7500 - val_loss: 0.4900 - val_accuracy: 0.6667\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4408 - accuracy: 0.7500 - val_loss: 0.4890 - val_accuracy: 0.7000\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4399 - accuracy: 0.7500 - val_loss: 0.4883 - val_accuracy: 0.7000\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.4392 - accuracy: 0.7500 - val_loss: 0.4870 - val_accuracy: 0.7000\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.4381 - accuracy: 0.7583 - val_loss: 0.4862 - val_accuracy: 0.7000\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.4372 - accuracy: 0.7583 - val_loss: 0.4852 - val_accuracy: 0.7000\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4363 - accuracy: 0.7583 - val_loss: 0.4843 - val_accuracy: 0.7333\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.4355 - accuracy: 0.7583 - val_loss: 0.4833 - val_accuracy: 0.7333\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4346 - accuracy: 0.7583 - val_loss: 0.4822 - val_accuracy: 0.7333\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 0.4337 - accuracy: 0.7583 - val_loss: 0.4811 - val_accuracy: 0.7333\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 212us/sample - loss: 0.4328 - accuracy: 0.7583 - val_loss: 0.4799 - val_accuracy: 0.7667\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4320 - accuracy: 0.7750 - val_loss: 0.4789 - val_accuracy: 0.7667\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4312 - accuracy: 0.7833 - val_loss: 0.4781 - val_accuracy: 0.7667\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4302 - accuracy: 0.7833 - val_loss: 0.4769 - val_accuracy: 0.7667\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 204us/sample - loss: 0.4294 - accuracy: 0.7917 - val_loss: 0.4755 - val_accuracy: 0.8333\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4286 - accuracy: 0.7917 - val_loss: 0.4746 - val_accuracy: 0.8333\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4277 - accuracy: 0.7917 - val_loss: 0.4736 - val_accuracy: 0.8333\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4270 - accuracy: 0.7917 - val_loss: 0.4721 - val_accuracy: 0.8333\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4260 - accuracy: 0.8000 - val_loss: 0.4710 - val_accuracy: 0.8667\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4252 - accuracy: 0.8083 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4244 - accuracy: 0.8083 - val_loss: 0.4692 - val_accuracy: 0.8667\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4236 - accuracy: 0.8083 - val_loss: 0.4682 - val_accuracy: 0.8667\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4228 - accuracy: 0.8083 - val_loss: 0.4675 - val_accuracy: 0.8667\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4219 - accuracy: 0.8083 - val_loss: 0.4667 - val_accuracy: 0.8667\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4211 - accuracy: 0.8083 - val_loss: 0.4656 - val_accuracy: 0.8667\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4203 - accuracy: 0.8167 - val_loss: 0.4648 - val_accuracy: 0.8667\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4195 - accuracy: 0.8167 - val_loss: 0.4641 - val_accuracy: 0.8667\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4187 - accuracy: 0.8167 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
      "Epoch 276/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4179 - accuracy: 0.8167 - val_loss: 0.4620 - val_accuracy: 0.8667\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4172 - accuracy: 0.8167 - val_loss: 0.4614 - val_accuracy: 0.8667\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4163 - accuracy: 0.8167 - val_loss: 0.4602 - val_accuracy: 0.8667\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4154 - accuracy: 0.8250 - val_loss: 0.4594 - val_accuracy: 0.8667\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4148 - accuracy: 0.8250 - val_loss: 0.4584 - val_accuracy: 0.8667\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4139 - accuracy: 0.8250 - val_loss: 0.4579 - val_accuracy: 0.8667\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4131 - accuracy: 0.8250 - val_loss: 0.4570 - val_accuracy: 0.8667\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4123 - accuracy: 0.8250 - val_loss: 0.4563 - val_accuracy: 0.8667\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4115 - accuracy: 0.8250 - val_loss: 0.4555 - val_accuracy: 0.8667\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4108 - accuracy: 0.8250 - val_loss: 0.4545 - val_accuracy: 0.8667\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4100 - accuracy: 0.8250 - val_loss: 0.4538 - val_accuracy: 0.8667\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4093 - accuracy: 0.8333 - val_loss: 0.4526 - val_accuracy: 0.8667\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4084 - accuracy: 0.8333 - val_loss: 0.4517 - val_accuracy: 0.8667\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4077 - accuracy: 0.8333 - val_loss: 0.4507 - val_accuracy: 0.8667\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4070 - accuracy: 0.8333 - val_loss: 0.4502 - val_accuracy: 0.8667\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 224us/sample - loss: 0.4062 - accuracy: 0.8333 - val_loss: 0.4492 - val_accuracy: 0.8667\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 0.4054 - accuracy: 0.8333 - val_loss: 0.4486 - val_accuracy: 0.8667\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4046 - accuracy: 0.8333 - val_loss: 0.4478 - val_accuracy: 0.8667\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 199us/sample - loss: 0.4039 - accuracy: 0.8333 - val_loss: 0.4470 - val_accuracy: 0.8667\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.4032 - accuracy: 0.8333 - val_loss: 0.4461 - val_accuracy: 0.8667\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.4023 - accuracy: 0.8333 - val_loss: 0.4454 - val_accuracy: 0.8667\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 324us/sample - loss: 0.4017 - accuracy: 0.8333 - val_loss: 0.4444 - val_accuracy: 0.8667\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.4008 - accuracy: 0.8333 - val_loss: 0.4437 - val_accuracy: 0.8667\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.4002 - accuracy: 0.8333 - val_loss: 0.4427 - val_accuracy: 0.9000\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.3994 - accuracy: 0.8333 - val_loss: 0.4419 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24867fef308>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = scaled_X_train, y = y_train, epochs = 300, validation_data = (scaled_X_test, y_test), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.172260</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.164368</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.167038</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.159729</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.161478</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>1.155266</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.156001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.150835</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.151039</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>1.146372</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.402327</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.445424</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.401659</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.400832</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.443686</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.400174</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.442674</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.399364</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.441880</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy\n",
       "0    1.172260  0.350000  1.164368      0.366667\n",
       "1    1.167038  0.350000  1.159729      0.366667\n",
       "2    1.161478  0.341667  1.155266      0.366667\n",
       "3    1.156001  0.333333  1.150835      0.366667\n",
       "4    1.151039  0.325000  1.146372      0.366667\n",
       "..        ...       ...       ...           ...\n",
       "295  0.402327  0.833333  0.445424      0.866667\n",
       "296  0.401659  0.833333  0.444444      0.866667\n",
       "297  0.400832  0.833333  0.443686      0.866667\n",
       "298  0.400174  0.833333  0.442674      0.900000\n",
       "299  0.399364  0.833333  0.441880      0.900000\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24868033d08>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x248683d9548>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['accuracy', 'val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4418804943561554, 0.9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_X_test, y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#44% loss and 90% accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for deployment we are gonna train on all the data, the full dataset as the test set has 90% accuracy which is pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclaed_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, activation = 'relu', input_shape = [4, ]))\n",
    "model.add(Dense(3, activation= 'softmax')) #last layer should always be equal to the number of classes\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/300\n",
      "150/150 [==============================] - 0s 3ms/sample - loss: 1.0780 - accuracy: 0.3333\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0696 - accuracy: 0.3333\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0625 - accuracy: 0.3333\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 1.0543 - accuracy: 0.3333\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0480 - accuracy: 0.3333\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0422 - accuracy: 0.3333\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0359 - accuracy: 0.3333\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0307 - accuracy: 0.3333\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 1.0252 - accuracy: 0.3333\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0198 - accuracy: 0.3333\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0151 - accuracy: 0.3333\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0101 - accuracy: 0.3333\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 1.0050 - accuracy: 0.3333\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0006 - accuracy: 0.3333\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9961 - accuracy: 0.3333\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9912 - accuracy: 0.3333\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9872 - accuracy: 0.3333\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9826 - accuracy: 0.3333\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9785 - accuracy: 0.3333\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.9744 - accuracy: 0.3333\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9701 - accuracy: 0.3400\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9659 - accuracy: 0.3400\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9618 - accuracy: 0.3400\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.9578 - accuracy: 0.3400\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9537 - accuracy: 0.3400\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.9494 - accuracy: 0.3400\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9457 - accuracy: 0.3467\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9418 - accuracy: 0.3533\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.9380 - accuracy: 0.3533\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9342 - accuracy: 0.3667\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.9305 - accuracy: 0.3667\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.9267 - accuracy: 0.3733\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.9230 - accuracy: 0.3867\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.9194 - accuracy: 0.4133\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.9160 - accuracy: 0.4267\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9124 - accuracy: 0.4400\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.9088 - accuracy: 0.4467\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9055 - accuracy: 0.4600\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9022 - accuracy: 0.4667\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8989 - accuracy: 0.4867\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8955 - accuracy: 0.5000\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8921 - accuracy: 0.5133\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8889 - accuracy: 0.5200\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8857 - accuracy: 0.5333\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8825 - accuracy: 0.5333\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8793 - accuracy: 0.5467\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8761 - accuracy: 0.5600\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8729 - accuracy: 0.5733\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8699 - accuracy: 0.5800\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8668 - accuracy: 0.5867\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8637 - accuracy: 0.5933\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.8606 - accuracy: 0.5933\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8575 - accuracy: 0.5933\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8545 - accuracy: 0.6000\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8515 - accuracy: 0.6067\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8485 - accuracy: 0.6133\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8456 - accuracy: 0.6133\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8425 - accuracy: 0.6133\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8395 - accuracy: 0.6133\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8367 - accuracy: 0.6133\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8337 - accuracy: 0.6333\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8308 - accuracy: 0.6333\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8280 - accuracy: 0.6333\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8250 - accuracy: 0.6400\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8221 - accuracy: 0.6467\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.8193 - accuracy: 0.6467\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8165 - accuracy: 0.6600\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.8136 - accuracy: 0.6600\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8109 - accuracy: 0.6600\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8082 - accuracy: 0.6733\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8053 - accuracy: 0.6800\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8026 - accuracy: 0.6800\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.7999 - accuracy: 0.6800\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7971 - accuracy: 0.6867\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7944 - accuracy: 0.6867\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7918 - accuracy: 0.6867\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7891 - accuracy: 0.6867\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7865 - accuracy: 0.6867\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7838 - accuracy: 0.6867\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.7812 - accuracy: 0.6867\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8153 - accuracy: 0.62 - 0s 93us/sample - loss: 0.7786 - accuracy: 0.6867\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7760 - accuracy: 0.6867\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7734 - accuracy: 0.6867\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7709 - accuracy: 0.6867\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7685 - accuracy: 0.6867\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.7659 - accuracy: 0.6867\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.7635 - accuracy: 0.6867\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.7611 - accuracy: 0.6933\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7587 - accuracy: 0.6933\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7563 - accuracy: 0.6933\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7540 - accuracy: 0.6933\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7515 - accuracy: 0.6933\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7492 - accuracy: 0.6933\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7469 - accuracy: 0.6933\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7445 - accuracy: 0.6933\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7423 - accuracy: 0.6933\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7399 - accuracy: 0.6933\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7376 - accuracy: 0.6933\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7355 - accuracy: 0.6933\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7332 - accuracy: 0.6933\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7311 - accuracy: 0.6933\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7289 - accuracy: 0.7000\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7268 - accuracy: 0.7000\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7245 - accuracy: 0.7000\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7224 - accuracy: 0.7000\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7203 - accuracy: 0.7000\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7182 - accuracy: 0.7000\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7161 - accuracy: 0.7000\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7141 - accuracy: 0.7000\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7121 - accuracy: 0.7000\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7100 - accuracy: 0.7000\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7080 - accuracy: 0.7067\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7061 - accuracy: 0.7067\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7041 - accuracy: 0.7067\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7021 - accuracy: 0.7067\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7002 - accuracy: 0.7067\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6983 - accuracy: 0.7067\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.7359 - accuracy: 0.75 - 0s 100us/sample - loss: 0.6963 - accuracy: 0.7067\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6944 - accuracy: 0.7067\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6926 - accuracy: 0.7067\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6906 - accuracy: 0.7067\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.75 - 0s 93us/sample - loss: 0.6887 - accuracy: 0.7067\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6869 - accuracy: 0.7133\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6851 - accuracy: 0.7133\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6832 - accuracy: 0.7133\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6814 - accuracy: 0.7133\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6796 - accuracy: 0.7133\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6778 - accuracy: 0.7133\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6760 - accuracy: 0.7133\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.59 - 0s 73us/sample - loss: 0.6743 - accuracy: 0.7133\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6725 - accuracy: 0.7133\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6707 - accuracy: 0.7133\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6690 - accuracy: 0.7200\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6673 - accuracy: 0.7200\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6655 - accuracy: 0.7267\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6637 - accuracy: 0.7267\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6621 - accuracy: 0.7267\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6603 - accuracy: 0.7267\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6586 - accuracy: 0.7267\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6569 - accuracy: 0.7267\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6551 - accuracy: 0.7267\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6535 - accuracy: 0.7267\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6518 - accuracy: 0.7267\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6501 - accuracy: 0.7267\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6484 - accuracy: 0.7267\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6469 - accuracy: 0.7267\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6452 - accuracy: 0.7267\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6435 - accuracy: 0.7267\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6419 - accuracy: 0.7267\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.6403 - accuracy: 0.7267\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6388 - accuracy: 0.7267\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6371 - accuracy: 0.7267\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6356 - accuracy: 0.7267\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6340 - accuracy: 0.7267\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.6325 - accuracy: 0.7267\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6309 - accuracy: 0.7267\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6294 - accuracy: 0.7333\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6280 - accuracy: 0.7333\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6264 - accuracy: 0.7333\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6249 - accuracy: 0.7333\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6235 - accuracy: 0.7333\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6219 - accuracy: 0.7333\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6204 - accuracy: 0.7333\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.6189 - accuracy: 0.7333\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.6174 - accuracy: 0.7333\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6159 - accuracy: 0.7400\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6145 - accuracy: 0.7400\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6132 - accuracy: 0.7333\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6116 - accuracy: 0.7400\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.6101 - accuracy: 0.7400\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6087 - accuracy: 0.7400\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6072 - accuracy: 0.7533\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.71 - 0s 93us/sample - loss: 0.6059 - accuracy: 0.7600\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.6044 - accuracy: 0.7600\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6029 - accuracy: 0.7600\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6015 - accuracy: 0.7600\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.6001 - accuracy: 0.7600\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5987 - accuracy: 0.7600\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5973 - accuracy: 0.7667\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.5959 - accuracy: 0.7667\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5946 - accuracy: 0.7667\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5932 - accuracy: 0.7800\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5917 - accuracy: 0.7800\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5904 - accuracy: 0.7800\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5891 - accuracy: 0.7800\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5877 - accuracy: 0.7800\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5863 - accuracy: 0.7800\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.5851 - accuracy: 0.7800\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.84 - 0s 100us/sample - loss: 0.5837 - accuracy: 0.7800\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5824 - accuracy: 0.7800\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5811 - accuracy: 0.7800\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5796 - accuracy: 0.7867\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5783 - accuracy: 0.7867\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5770 - accuracy: 0.7867\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5757 - accuracy: 0.7933\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5744 - accuracy: 0.7933\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5731 - accuracy: 0.7933\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5718 - accuracy: 0.7933\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.5705 - accuracy: 0.8000\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5691 - accuracy: 0.8067\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5679 - accuracy: 0.8067\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5667 - accuracy: 0.8067\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5653 - accuracy: 0.8133\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5641 - accuracy: 0.8133\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5628 - accuracy: 0.8133\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5614 - accuracy: 0.8133\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5602 - accuracy: 0.8200\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5590 - accuracy: 0.8200\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5577 - accuracy: 0.8200\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5564 - accuracy: 0.8200\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5551 - accuracy: 0.8267\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5538 - accuracy: 0.8267\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.5525 - accuracy: 0.8267\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5513 - accuracy: 0.8267\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5500 - accuracy: 0.8333\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5488 - accuracy: 0.8333\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.5476 - accuracy: 0.8333\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5463 - accuracy: 0.8333\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5451 - accuracy: 0.8333\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5438 - accuracy: 0.8333\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5426 - accuracy: 0.8333\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5414 - accuracy: 0.8333\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5403 - accuracy: 0.8333\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5389 - accuracy: 0.8333\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5378 - accuracy: 0.8333\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5365 - accuracy: 0.8333\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5352 - accuracy: 0.8333\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5341 - accuracy: 0.8333\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5329 - accuracy: 0.8333\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5316 - accuracy: 0.8400\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5304 - accuracy: 0.8400\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5291 - accuracy: 0.8400\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5280 - accuracy: 0.8400\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5267 - accuracy: 0.8467\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5256 - accuracy: 0.8467\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5243 - accuracy: 0.8467\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5231 - accuracy: 0.8467\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5219 - accuracy: 0.8467\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5207 - accuracy: 0.8467\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5195 - accuracy: 0.8467\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5183 - accuracy: 0.8467\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5171 - accuracy: 0.8467\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5160 - accuracy: 0.8467\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5147 - accuracy: 0.8467\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.5135 - accuracy: 0.8467\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5124 - accuracy: 0.8467\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5112 - accuracy: 0.8533\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5101 - accuracy: 0.8533\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5088 - accuracy: 0.8533\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5076 - accuracy: 0.8533\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5064 - accuracy: 0.8533\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5052 - accuracy: 0.8533\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5041 - accuracy: 0.8533\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5030 - accuracy: 0.8533\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5017 - accuracy: 0.8533\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 219us/sample - loss: 0.5005 - accuracy: 0.8533\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.4994 - accuracy: 0.8533\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4982 - accuracy: 0.8533\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4971 - accuracy: 0.8533\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4960 - accuracy: 0.8533\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4948 - accuracy: 0.8533\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.4936 - accuracy: 0.8533\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4926 - accuracy: 0.8533\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4913 - accuracy: 0.8533\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4902 - accuracy: 0.8533\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4891 - accuracy: 0.8533\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4880 - accuracy: 0.8533\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4868 - accuracy: 0.8533\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4857 - accuracy: 0.8533\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4845 - accuracy: 0.8533\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4834 - accuracy: 0.8533\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4822 - accuracy: 0.8533\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.4811 - accuracy: 0.8600\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4801 - accuracy: 0.8667\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4790 - accuracy: 0.8667\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4778 - accuracy: 0.8667\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4766 - accuracy: 0.8667\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4754 - accuracy: 0.8733\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.4744 - accuracy: 0.8733\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4733 - accuracy: 0.8867\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4722 - accuracy: 0.8933\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4713 - accuracy: 0.9000\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4700 - accuracy: 0.9000\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4690 - accuracy: 0.9000\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4678 - accuracy: 0.9000\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4667 - accuracy: 0.9000\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4656 - accuracy: 0.9000\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4646 - accuracy: 0.9000\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4634 - accuracy: 0.9000\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4624 - accuracy: 0.9000\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4613 - accuracy: 0.9067\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4603 - accuracy: 0.9067\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4592 - accuracy: 0.9067\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4582 - accuracy: 0.9067\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4571 - accuracy: 0.9067\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4560 - accuracy: 0.9067\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4549 - accuracy: 0.9067\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4538 - accuracy: 0.9067\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4529 - accuracy: 0.9067\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4517 - accuracy: 0.9133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2486a0af5c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sclaed_X, y, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('iris_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scaler....new data needs to be scaled in the way model expects it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_scaler.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, 'iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on a new single flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_model = load_model('final_iris_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_scaler = joblib.load('iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll need our input in json format to predict with an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(1)\n",
    "#taking one of the datapoints to show how it'll work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example = {'sepal_length':5.1,\n",
    "                 'sepal_width':3.5,\n",
    "                 'petal_length':1.4,\n",
    "                 'petal_width':0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flower_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_ #one hot encoded classes (setosa at index 0 , versi at 1 and virginica at 2)(actual index locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    # For larger data features, you should probably write a for loop\n",
    "    # That builds out this array for you\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]] #assigning an array because that's how the model expects it\n",
    "    \n",
    "    flower = scaler.transform(flower) #scaling the flower\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)[0] #we get the actual class index, instead of array holding the class index\n",
    "    \n",
    "    return classes[class_ind] #returns the class at the class_ind\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_prediction(flower_model,flower_scaler,flower_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "\n",
    "flower_model = load_model(\"final_iris_model.h5\")\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")\n",
    "\n",
    "\n",
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    # For larger data features, you should probably write a for loop\n",
    "    # That builds out this array for you\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)\n",
    "    \n",
    "    return classes[class_ind][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
